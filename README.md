# Mini-Llama2
custom impelementation of llama2 without kv cache , i used imdb dataset for training
if you want to train the model you can play with paramters of optimizer and embedding dim , and training epochs without touching tokenizer and dataloader
